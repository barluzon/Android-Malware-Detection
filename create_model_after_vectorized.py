from math import inf
import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
import os
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest

# Set pandas to show all columns when you print a dataframe
pd.set_option('display.max_columns', None)

dataset_path_csv = 'out_vectorized_csv.csv'
df = pd.read_csv(dataset_path_csv)
print('df shape:', df.shape)

def feature_selection(dataframe):
    X = dataframe.drop(['label'], axis=1)
    columns = X.columns
    X = np.nan_to_num(X)
    y = dataframe['label']
    X = (X - X.min()) / (X.max() - X.min())

    # k tells k top features to be selected
    # Score function Chi2 tells the feature to be selected using Chi Square
    test = SelectKBest(score_func=chi2, k=1000)
    fit = test.fit(X, y)

    delete_features = pd.DataFrame({'columns': columns, 'Kept': test.get_support()})
    delete_features = delete_features[(delete_features['Kept'] == False)]['columns'].to_list()

    return delete_features


from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier

def feature_selection_recursive(dataframe):
    X = dataframe.drop(['label'], axis=1)
    print('starting feature selection recursively...')
    columns = X.columns
    X = np.nan_to_num(X)
    y = dataframe['label']
    # y = dataframe['attack_type']
    X = (X - X.min()) / (X.max() - X.min())

    min_features_to_select = 100  # Minimum number of features to consider
    clf = RandomForestClassifier()
    cv = StratifiedKFold(5)

    rfecv = RFECV(
        estimator=clf,
        step=1,
        cv=cv,
        scoring="accuracy",
        min_features_to_select=min_features_to_select,
        n_jobs=2,
    )
    rfecv.fit(X, y)

    delete_features = pd.DataFrame({'columns': columns, 'Kept': rfecv.get_support()})
    delete_features = delete_features[(delete_features['Kept'] == False)]['columns'].to_list()

    return delete_features


delete_features = feature_selection(df)
dataset = df.drop(delete_features, axis=1)

features_list = dataset.columns.to_list()
features_list.remove('label')

print('features list len: ', len(features_list))

# with open(r'select_features/best10000_features.txt', 'w') as fp:
#     for item in features_list:
#         # write each item on a new line
#         fp.write("%s\n" % item)
#     print('Done')

"""## Train test split"""

X = dataset[features_list].to_numpy()
y = np.stack(dataset['label'])
print("x shape: ", X.shape)
print("y shape: ", y.shape)

"""## Model fitting"""
from sec_svm import CClassifierSecSVM
from sec_svm_V2 import SecSVMClassifier


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)

print('X train shape', X_train.shape, 'y train shape', y_train.shape)
print('X test shape', X_test.shape, 'y test shape', y_test.shape)

#clf = CClassifierSecSVM(ub=inf, idx_ub=None, lb=-inf, idx_lb=None, eta=0.5, max_it=10000.0, eps=0.0001)
clf = SecSVMClassifier(upper_bound=inf, idx_ub=None, lower_bound=-inf, idx_lb=None, step_size=0.5, max_iterations=10000.0, eps=0.0001)
clf.fit(X_train, y_train)

# Save the model
import pickle

pickle.dump(clf, open(r'models/bar_sec_svm_model_1000.pkl', 'wb'))

# We print our results
sns.set(rc={'figure.figsize': (15, 8)})
pred = clf.predict(X_test)
true_labels = y_test
predictions = []
for i in pred:
    predictions.append(i)
cf_matrix = confusion_matrix(true_labels, predictions)
clf_report = classification_report(true_labels, predictions, digits=5)
heatmap = sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='g',
                      xticklabels=np.unique(true_labels),
                      yticklabels=np.unique(true_labels))

# The heatmap is cool but this is the most important result
print(clf_report)
