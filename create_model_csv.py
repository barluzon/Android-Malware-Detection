from math import inf
import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
import os

# Set pandas to show all columns when you print a dataframe
pd.set_option('display.max_columns', None)

# Global setting here you choose the dataset number and classification type for the model

#dataset_path_csv = "data_3_mix.csv"
dataset_path_csv = 'out_after_delete_23k.csv'
df = pd.read_csv(dataset_path_csv)

# drop some samples to make data balance
# df_benign.drop(df_benign.tail(1560).index, inplace=True)
print('df shape:', df.shape)
# for column in df.columns[df.isna().any()].tolist():
#    df[column] = df[column].fillna(0)


# Iterate through the data set and check if there is column that contains one value
from collections import Counter


def non_unique_features(dataframe):
    same_value_features = []
    for col in dataframe.columns:
        if col != 'label':
            if len(dataframe[col].unique()) == 1:
                same_value_features.append(col)
    return same_value_features


# for seperated df
#df_feat_to_delete = non_unique_features(df)
df_feat_to_delete =[]
print('df_feat_to_delete: ', df_feat_to_delete)

df = df.drop(df_feat_to_delete, axis=1)


# delete all columns with over 70% null values
def most_null_features(dataset):
    col_to_del = []
    for col in dataset.columns:
        if ((dataset.isnull()[str(col)].mean().round(4) * 100) > 70):
            col_to_del.append(col)
    print('number of features to delete: ', len(col_to_del))
    return col_to_del


# for seperated df
#temp_list1 = most_null_features(df)
#for i in temp_list1:
#    df_feat_to_delete.append(i)

df.drop(df_feat_to_delete, axis=1, inplace=True, errors='ignore')

for column in df.columns[df.isna().any()].tolist():
    df[column] = df[column].fillna(0)

# drop files names
df.drop('sha256', axis=1, inplace=True, errors='ignore')
print('df num of features: ', len(df.columns))
import os

#df.to_csv('csv/out_after_delete_csv.csv')

means = df.isnull().mean()
percentages = means[means > 0].round(4).mul(100).sort_values(ascending=False)
print('percentages:', percentages)

# Setting features for further feature extraction by choosing columns
# Some will be "simply" encoded via label encoding and others with HashingVectorizer
from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from collections import Counter


def vectorize_df(df):
    le = LabelEncoder()
    for col in df.columns:
        df[col] = le.fit_transform(df[col].astype(str))
    return df


dataset = vectorize_df(df)

dataset.to_csv('out_vectorized_csv.csv')

print('number of features: ', len(dataset.columns))

##############################################################
#run from here after have vectorized csv

###only if run vectorized
# dataset_path_csv = 'out_vectorized_csv.csv.csv'
# dataset = pd.read_csv(dataset_path_csv)
###

from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest


def feature_selection(dataframe):
    X = dataframe.drop(['label'], axis=1)
    print('x is:', X.columns.to_list())
    columns = X.columns
    X = np.nan_to_num(X)
    y = dataframe['label']
    X = (X - X.min()) / (X.max() - X.min())

    # k tells k top features to be selected
    # Score function Chi2 tells the feature to be selected using Chi Square
    test = SelectKBest(score_func=chi2, k=3000)
    fit = test.fit(X, y)

    delete_features = pd.DataFrame({'columns': columns, 'Kept': test.get_support()})
    print(delete_features)
    delete_features = delete_features[(delete_features['Kept'] == False)]['columns'].to_list()

    return delete_features


from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier


def feature_selection_recursive(dataframe):
    X = dataframe.drop(['label'], axis=1)
    print('starting feature selection recursively...')
    columns = X.columns
    X = np.nan_to_num(X)
    y = dataframe['label']
    # y = dataframe['attack_type']
    X = (X - X.min()) / (X.max() - X.min())

    min_features_to_select = 100  # Minimum number of features to consider
    clf = RandomForestClassifier()
    cv = StratifiedKFold(5)

    rfecv = RFECV(
        estimator=clf,
        step=1,
        cv=cv,
        scoring="accuracy",
        min_features_to_select=min_features_to_select,
        n_jobs=2,

    )
    rfecv.fit(X, y)

    delete_features = pd.DataFrame({'columns': columns, 'Kept': rfecv.get_support()})
    delete_features = delete_features[(delete_features['Kept'] == False)]['columns'].to_list()
    print(delete_features)

    return delete_features


delete_features = feature_selection(dataset)
dataset = dataset.drop(delete_features, axis=1)

features_list = dataset.columns.to_list()
features_list.remove('label')

print('features list len: ', len(features_list))

with open(r'csv_features.txt', 'w') as fp:
    for item in features_list:
        # write each item on a new line
        fp.write("%s\n" % item)
    print('Done')

"""## Train test split"""

X = dataset[features_list].to_numpy()
y = np.stack(dataset['label'])
print("x shape: ", X.shape)
print("y shape: ", y.shape)

"""## Model fitting"""


from secml.ml.classifiers import CClassifierSVM
from sec_svm import SecSVMClassifier

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)

print('X train shape', X_train.shape, 'y train shape', y_train.shape)
print('X test shape', X_test.shape, 'y test shape', y_test.shape)

clf = SecSVMClassifier(upper_bound=inf, idx_ub=None, lower_bound=-inf, idx_lb=None, step_size=0.5, max_iterations=10000.0, eps=0.0001)
clf.fit(X_train, y_train)

# Save the model
import pickle

pickle.dump(clf, open(r'sec_svm_model_1000.pkl', 'wb'))

# We print our results
sns.set(rc={'figure.figsize': (15, 8)})
pred = clf.predict(X_test)
true_labels = y_test
predictions = []
for i in pred:
    predictions.append(i)
cf_matrix = confusion_matrix(true_labels, predictions)
clf_report = classification_report(true_labels, predictions, digits=5)
heatmap = sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='g',
                      xticklabels=np.unique(true_labels),
                      yticklabels=np.unique(true_labels))

# The heatmap is cool but this is the most important result
print(clf_report)
