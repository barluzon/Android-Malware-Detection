# Importing necessary modules
import numpy as np
from secml.array import CArray
from secml.ml.classifiers import CClassifierSVM
from secml.ml.classifiers.clf_utils import convert_binary_labels
from secml.core.constants import inf


class SecSVMClassifier(CClassifierSVM):
    def __init__(self, upper_bound=inf, idx_ub=None, lower_bound=-inf, idx_lb=None,
                 step_size=0.5, max_iterations=1e4, eps=1e-4):  # was with , *args, **kwargs

        # Initialize the parent class
        super(self.__class__, self).__init__()  # was with *args, **kwargs

        # Set training parameters
        self.step_size = step_size
        self.max_iterations = max_iterations
        self.eps = eps

        # Check and set gradient bounds
        if (CArray(upper_bound) < CArray(lower_bound)).any():
            raise ValueError("upper_bound should be higher than lower_bound")

        self.upper_bound = upper_bound
        if idx_ub is not None:
            self.idx_ub = idx_ub

        self.lower_bound = lower_bound
        if idx_lb is not None:
            self.idx_lb = idx_lb

    @property
    def upper_bound(self):
        """Return value of weight upper bound"""
        return self._ub

    @upper_bound.setter
    def upper_bound(self, value):
        """Set value of weight upper bound"""
        self._ub = value

    @property
    def lower_bound(self):
        """Return value of weight lower bound"""
        return self._lb

    @lower_bound.setter
    def lower_bound(self, value):
        """Set value of weight lower bound"""
        self._lb = value

    @property
    def weights(self):
        """Return the vector of feature weights (only if ckernel is None)"""
        return self._w

    @property
    def bias(self):
        """Return the SVM bias (b term in the decision function)"""
        return self._b

    @property
    def max_iterations(self):
        """Maximum number of iterations for training"""
        return self._max_iterations

    @max_iterations.setter
    def max_iterations(self, value):
        """Set maximum number of iterations for training"""
        self._max_iterations = int(value)

    @property
    def step_size(self):
        """Step size parameter for the training gradient"""
        return self._step_size

    @step_size.setter
    def step_size(self, value):
        """Set step size parameter for the training gradient"""
        self._step_size = float(value)

    @property
    def eps(self):
        """eps for the stopping criterion of the training gradient"""
        return self._eps

    @eps.setter
    def eps(self, value):
        """Set eps for the stopping criterion of the training gradient"""
        self._eps = value


    def hinge_loss(self, x, y):
        """Compute the loss term for each point in dataset."""
        score = self.decision_function(x, y=1)
        loss = 1.0 - y * score
        loss[loss < 0] = 0.0
        return loss

    def C_hinge_loss(self, x, y):
        """Compute the loss term for each point in dataset multiplied by C.

        If class_weight == 'balanced', it multiplies C to the inverse
        prob of theclasses.

        """

        loss = self.hinge_loss(x, y)
        if self.class_weight == 'balanced':
            loss *= self.weight[1] if y == 1 else self.weight[0]
        return self.C * loss

    def gradient_w_b(self, x, y):
        """
        Compute the gradient dloss/dw, where loss is \sum_i max(0, 1-y_i*f(x_i))
        """

        loss = self.hinge_loss(x, y)  # loss(y,f(x))

        idx_err_vect = loss > 0

        grad_b = -self.C * y[idx_err_vect].sum()

        grad_loss = CArray.zeros(x.shape[1])
        if (idx_err_vect * (y < 0)).any():
            grad_loss += x[idx_err_vect * (y <= 0), :].sum(axis=0, keepdims=False)
        if (idx_err_vect * (y > 0)).any():
            grad_loss -= x[idx_err_vect * (y > 0), :].sum(axis=0, keepdims=False)

        grad_w = self.w + self.C * grad_loss

        return grad_w, grad_b


    def objective(self, x, y):
        """Objective function."""

        return 0.5 * (self.weights**2).sum() + self.C_hinge_loss(x, y).sum()


    def _fit(self, x, y):

        if self.n_classes != 2:
            raise ValueError("SVM only works for binary classification")
        y = convert_binary_labels(y)
        self._w = CArray.zeros(x.shape[1])
        self._b = CArray(0.0)
        obj = self.objective(x, y)
        for i in range(self.max_iterations):
            idx = CArray.randsample(CArray.arange(x.shape[0], dtype=int), x.shape[0], random_state=i)
            grad_w, grad_b = self.gradient_w_b(x[idx, :], y[idx])
            step = self.step_size / (x.shape[0] ** 0.5)
            self._w -= step * grad_w
            self._b -= step * grad_b
            if self.class_weight == 'balanced':
                n_pos = y[y == 1].shape[0]
                n_neg = y[y == -1].shape[0]
                self.weight = CArray.zeros(2)
                self.weight[0] = 1.0 * n_pos / (n_pos + n_neg)
                self.weight[1] = 1.0 * n_neg / (n_pos + n_neg)
            obj_new = self.objective(x, y)
            if abs(obj_new - obj) < self.eps:
                self.logger.info("i {:}: {:}".format(i, obj_new))
                self._w = self.weights.tosparse() if x.issparse else self.weights
                return
            obj = obj_new
            if i % 10 == 0:
                loss = self.hinge_loss(x, y).sum()
                self.logger.info("i {:}: {:.4f}, L {:.4f}".format(i, obj, loss))
        self._w = self.weights.tosparse() if x.issparse else self.weights
