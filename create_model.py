from math import inf
import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
import os

# Set pandas to show all columns when you print a dataframe
pd.set_option('display.max_columns', None)

# Global setting here you choose the dataset number and classification type for the model
dataset_path_malwares = "malwares1800.json"
dataset_path_benign = "benign1800.json"

# dataset_csv = "/content/drive/MyDrive/AndroidAttack/Data/csv/out.csv";

# Read the json and read it to a pandas dataframe object, you can change these settings
with open(dataset_path_malwares) as file:
    raw_ds = json.load(file)
df_malwares = pd.json_normalize(raw_ds, max_level=2)

with open(dataset_path_benign) as file:
    raw_ds = json.load(file)
df_benign = pd.json_normalize(raw_ds, max_level=2)

# drop some samples to make data balance
# df_benign.drop(df_benign.tail(1560).index, inplace=True)
print('benign shape:', df_benign.shape)
print('malwares shape:', df_malwares.shape)

# for column in df_malwares.columns[df_malwares.isna().any()].tolist():
#    df_malwares[column] = df_malwares[column].fillna(0)

# for column in df_benign.columns[df_benign.isna().any()].tolist():
#    df_benign[column] = df_benign[column].fillna(0)

df_malwares['label'] = 1
df_benign['label'] = 0

# df.to_csv('/csv/out.csv')
# df = pd.read_csv(dataset_csv)

# Iterate through the data set and check if there is column that contains one value
from collections import Counter


def non_unique_features(dataframe):
    same_value_features = []
    for col in dataframe.columns:
        if col != 'label':
            if len(dataframe[col].unique()) == 1:
                same_value_features.append(col)
    return same_value_features


# for seperated df
benign_feat_to_delete = non_unique_features(df_benign)
malwares_feat_to_delete = non_unique_features(df_malwares)

print('benign_feat_to_delete:', benign_feat_to_delete)
print('malwares_feat_to_delete:', malwares_feat_to_delete)

df_benign = df_benign.drop(benign_feat_to_delete, axis=1)
df_malwares = df_malwares.drop(malwares_feat_to_delete, axis=1)


# delete all columns with over 98.5% null values
def most_null_features(dataset):
    col_to_del = []
    for col in dataset.columns:
        if ((dataset.isnull()[str(col)].mean().round(4) * 100) > 98.5):
            col_to_del.append(col)
    print('number of features to delete: ', len(col_to_del))
    return col_to_del


# for seperated df
temp_list1 = most_null_features(df_benign)
for i in temp_list1:
    benign_feat_to_delete.append(i)

temp_list2 = most_null_features(df_malwares)
for i in temp_list2:
    malwares_feat_to_delete.append(i)

df_benign.drop(benign_feat_to_delete, axis=1, inplace=True, errors='ignore')
df_malwares.drop(malwares_feat_to_delete, axis=1, inplace=True, errors='ignore')

# combine 2 df to 1 df
df = df_benign.append(df_malwares, ignore_index=True)

for column in df.columns[df.isna().any()].tolist():
    df[column] = df[column].fillna(0)

# drop files names
df.drop('sha256', axis=1, inplace=True)

import os

df.to_csv('csv/out_after_delete.csv')

means = df.isnull().mean()
percentages = means[means > 0].round(4).mul(100).sort_values(ascending=False)
print('percentages:', percentages)

# Setting features for further feature extraction by choosing columns
# Some will be "simply" encoded via label encoding and others with HashingVectorizer
from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from collections import Counter


def vectorize_df(df):
    le = LabelEncoder()
    for column in df.columns:
        df[column] = le.fit_transform(df[column].astype(str))
    return df


dataset = vectorize_df(df)

print('number of features: ', len(dataset.columns))

from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest


def feature_selection(dataframe):
    X = dataframe.drop(['label'], axis=1)
    print('x is:', X.columns.to_list())
    columns = X.columns
    X = np.nan_to_num(X)
    y = dataframe['label']
    X = (X - X.min()) / (X.max() - X.min())

    # k tells k top features to be selected
    # Score function Chi2 tells the feature to be selected using Chi Square
    test = SelectKBest(score_func=chi2, k=1000)
    fit = test.fit(X, y)

    delete_features = pd.DataFrame({'columns': columns, 'Kept': test.get_support()})
    print(delete_features)
    delete_features = delete_features[(delete_features['Kept'] == False)]['columns'].to_list()

    return delete_features


from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier


def feature_selection_recursive(dataframe):
    X = dataframe.drop(['label'], axis=1)
    print('starting feature selection recursively...')
    columns = X.columns
    X = np.nan_to_num(X)
    y = dataframe['label']
    # y = dataframe['attack_type']
    X = (X - X.min()) / (X.max() - X.min())

    min_features_to_select = 100  # Minimum number of features to consider
    clf = RandomForestClassifier()
    cv = StratifiedKFold(5)

    rfecv = RFECV(
        estimator=clf,
        step=1,
        cv=cv,
        scoring="accuracy",
        min_features_to_select=min_features_to_select,
        n_jobs=2,

    )
    rfecv.fit(X, y)

    delete_features = pd.DataFrame({'columns': columns, 'Kept': rfecv.get_support()})
    delete_features = delete_features[(delete_features['Kept'] == False)]['columns'].to_list()
    print(delete_features)

    return delete_features


delete_features = feature_selection(dataset)
dataset = dataset.drop(delete_features, axis=1)

features_list = dataset.columns.to_list()
features_list.remove('label')

print('features list: ', features_list)
print('features list len: ', len(features_list))

with open(r'best1000_features.txt', 'w') as fp:
    for item in features_list:
        # write each item on a new line
        fp.write("%s\n" % item)
    print('Done')

"""## Train test split"""

X = dataset[features_list].to_numpy()
y = np.stack(dataset['label'])
print("x shape: ", X.shape)
print("y shape: ", y.shape)

"""## Model fitting"""

from sklearn.svm import LinearSVC
from sklearn.svm import SVC
from secml.ml.classifiers import CClassifierSVM
from sec_svm import CClassifierSecSVM

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)

print('X train shape', X_train.shape, 'y train shape', y_train.shape)
print('X test shape', X_test.shape, 'y test shape', y_test.shape)

# clf = CClassifierSVM(C= 1, kernel = 'linear')
clf = CClassifierSecSVM(ub=inf, idx_ub=None, lb=-inf, idx_lb=None, eta=0.5, max_it=10000.0, eps=0.0001)
clf.fit(X_train, y_train)

# Save the model
import pickle

pickle.dump(clf, open(r'sec_svm_model.pkl', 'wb'))

# We print our results
sns.set(rc={'figure.figsize': (15, 8)})
pred = clf.predict(X_test)
true_labels = y_test
predictions = []
for i in pred:
    predictions.append(i)
cf_matrix = confusion_matrix(true_labels, predictions)
clf_report = classification_report(true_labels, predictions, digits=5)
heatmap = sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='g',
                      xticklabels=np.unique(true_labels),
                      yticklabels=np.unique(true_labels))

# The heatmap is cool but this is the most important result
print(clf_report)
